
- 언어 모델 확장자: .gguf
- 언어 모델: TinyLlama 1.1B(허깅 페이스에서 기본 모델 쳇지피티에게 물어봐서 다운받음)
- 파인 튜닝 방식: QLoRA 
- 언어 모델 훈련하는 곳: 허깅 페이스
- 허깅 페이스에서 모델을 가지고 올 때, 토큰을 발급 받아야 함.
#### (프롬프팅)
코랩에서 T4 설정까지 마친 후 이제 허깅페이스에 있는 타이니라마 모델로 랭체인을 작성하려고 해. 단계별로 설명해줘. TinyLlama 1.1B + QLoRA 방식 파인튜닝



#### 라이브러리 설치 관련 명령어 순서(먼저 코랩에서 실행되려면 환경변수부터 설정해야한다.)
1. 아래 부분 코랩에 환경변수 코드 입력하기
!pip install -qU transformers accelerate bitsandbytes langchain sentencepiece  
!pip install torch==2.6.0  
!pip install langchain_community
#### 추가로, 허깅 페이스 토큰 발급을 해야한다. 허깅 페이스에서 토큰 발급 받는 방법을 쳇 지피티에 물어보면 된다
langchain-token: [YOUR_HUGGINGFACE_TOKEN_HERE]

2. 아래 부분은 토큰 관련 명령어이다. 여기에 토큰 발급받은 것을 넣는 공간이 있다. 여기 명령어에 추가하면 된다.
from huggingface_hub import notebook_login  
notebook_login()

3. 아래 부분 코랩에 코드 입력하기
import torch  
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig# QLoRA 설정 (4비트 양자화)  
bnb_config = BitsAndBytesConfig(  
    load_in_4bit=True,  
    bnb_4bit_quant_type="nf4",                # 양자화 타입 (nf4 추천)  
    bnb_4bit_compute_dtype=torch.bfloat16,    # 계산 시 사용할 데이터 타입 (T4는 bfloat16 지원)  
    bnb_4bit_use_double_quant=False,          # 이중 양자화 사용 여부  
)# 모델 ID  
model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"# 토크나이저 로드  
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama)  
#패딩 토큰이 없다면 추가 (일부 모델은 필요)  
if tokenizer.pad_token is None:  
    tokenizer.pad_token = tokenizer.eos_token# 모델 로드 (4비트 양자화 적용)  
model = AutoModelForCausalLM.from_pretrained(  
    model_id,  
    quantization_config=bnb_config,  
    torch_dtype=torch.bfloat16, # 로드 시에도 bfloat16 사용 명시 (T4와 호환)  
    device_map="auto",          # 사용 가능한 GPU에 모델 레이어를 자동으로 분배  
    trust_remote_code=True,     # 일부 모델은 커스텀 코드가 있을 수 있음  
)print("모델 및 토크나이저 로드 완료!")  
print(f"사용 중인 디바이스: {model.device}")

4. 아래 부분 코랩에 코드 입력하기

from transformers import pipeline# LangChain과 연동하기 위한 텍스트 생성 파이프라인 생성  
pipe = pipeline(  
    task="text-generation",  
    model=model,  
    tokenizer=tokenizer,  
    max_new_tokens=256,  
    temperature=0.6,  # 0.3 ~ 0.7 사이에서 조절해 보세요. 너무 높으면 부정확해질 수 있습니다.  
    repetition_penalty=1.15, # 반복을 줄이기 위해 조금 더 높일 수 있습니다.  
    do_sample=True,     # <<< 중요! 샘플링 사용  
    top_k=50,           # 다음 토큰 선택 시 고려할 상위 k개 (선택 사항)  
    top_p=0.95,         # 다음 토큰 선택 시 고려할 누적 확률 (선택 사항)  
    pad_token_id=tokenizer.eos_token_id  
)print("Transformers 파이프라인 생성 완료!")


5. 아래 부분 코랩에 코드 입력하기
from langchain.llms import HuggingFacePipeline  
from langchain.prompts import PromptTemplate  
from langchain.chains import LLMChain# LangChain LLM 객체 생성  
llm = HuggingFacePipeline(pipeline=pipe)# 프롬프트 템플릿 정의  
#TinyLlama-Chat 모델은 보통 Alpaca 스타일의 프롬프트를 사용합니다.  
#<|system|>  
#You are a friendly chatbot.</s>  
#<|user|>  
#{instruction}</s>  
#<|assistant|>  
#아래는 단순화된 예시입니다. 실제 모델의 프롬프트 형식을 확인하고 적용하는 것이 좋습니다.template = """<|system|>  
You are a helpful and friendly AI assistant. Your primary goal is to understand the user's question in Korean and provide an accurate, concise, and natural-sounding answer in Korean.</s>  
<|user|>  
{question}</s>  
<|assistant|>  
"""  
prompt = PromptTemplate(template=template, input_variables=["question"])# LLMChain 생성  
llm_chain = LLMChain(prompt=prompt, llm=llm)print("LangChain LLMChain 생성 완료!")# LLMChain 실행  
question = "대한민국의 수도는 어디인가요? 그리고 그곳의 유명한 관광지 3곳을 알려주세요."  
response = llm_chain.run(question) # 또는 llm_chain.invoke({"question": question})print(f"\n질문: {question}")  
print(f"답변: {response}")question_2 = "양자컴퓨팅에 대해 초등학생도 이해할 수 있게 설명해줘."  
response_2 = llm_chain.run(question_2)  
print(f"\n질문: {question_2}")  
print(f"답변: {response_2}")



#### 허깅페이스에  코랩에서 훈련된 모델을 등록하는 방법(지금까지 했던 것 명령어 다 주고, 아래에 핑크색으로 강조한 부분만 추가해서 올리면 됨)
#### (프롬프팅)
!pip install torch==2.6.0 !pip install langchain_community from huggingface_hub import notebook_login notebook_login() import torch from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # QLoRA 설정 (4비트 양자화) bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type="nf4", # 양자화 타입 (nf4 추천) bnb_4bit_compute_dtype=torch.bfloat16, # 계산 시 사용할 데이터 타입 (T4는 bfloat16 지원) bnb_4bit_use_double_quant=False, # 이중 양자화 사용 여부 ) # 모델 ID model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0" # 토크나이저 로드 tokenizer = AutoTokenizer.from_pretrained(model_id) # 패딩 토큰이 없다면 추가 (일부 모델은 필요) if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token # 모델 로드 (4비트 양자화 적용) model = AutoModelForCausalLM.from_pretrained( model_id, quantization_config=bnb_config, torch_dtype=torch.bfloat16, # 로드 시에도 bfloat16 사용 명시 (T4와 호환) device_map="auto", # 사용 가능한 GPU에 모델 레이어를 자동으로 분배 trust_remote_code=True, # 일부 모델은 커스텀 코드가 있을 수 있음 ) print("모델 및 토크나이저 로드 완료!") print(f"사용 중인 디바이스: {model.device}") from transformers import pipeline # LangChain과 연동하기 위한 텍스트 생성 파이프라인 생성 pipe = pipeline( task="text-generation", model=model, tokenizer=tokenizer, max_new_tokens=256, temperature=0.6, # 0.3 ~ 0.7 사이에서 조절해 보세요. 너무 높으면 부정확해질 수 있습니다. repetition_penalty=1.15, # 반복을 줄이기 위해 조금 더 높일 수 있습니다. do_sample=True, # <<< 중요! 샘플링 사용 top_k=50, # 다음 토큰 선택 시 고려할 상위 k개 (선택 사항) top_p=0.95, # 다음 토큰 선택 시 고려할 누적 확률 (선택 사항) pad_token_id=tokenizer.eos_token_id ) print("Transformers 파이프라인 생성 완료!") from langchain.llms import HuggingFacePipeline from langchain.prompts import PromptTemplate from langchain.chains import LLMChain # LangChain LLM 객체 생성 llm = HuggingFacePipeline(pipeline=pipe) # 프롬프트 템플릿 정의 # TinyLlama-Chat 모델은 보통 Alpaca 스타일의 프롬프트를 사용합니다. # <|system|> # You are a friendly chatbot.</s> # <|user|> # {instruction}</s> # <|assistant|> # 아래는 단순화된 예시입니다. 실제 모델의 프롬프트 형식을 확인하고 적용하는 것이 좋습니다. template = """<|system|> You are a helpful and friendly AI assistant. Your primary goal is to understand the user's question in Korean and provide an accurate, concise, and natural-sounding answer in Korean.</s> <|user|> {question}</s> <|assistant|> """ prompt = PromptTemplate(template=template, input_variables=["question"]) # LLMChain 생성 llm_chain = LLMChain(prompt=prompt, llm=llm) print("LangChain LLMChain 생성 완료!") # LLMChain 실행 question = "대한민국의 수도는 어디인가요? 그리고 그곳의 유명한 관광지 3곳을 알려주세요." response = llm_chain.run(question) # 또는 llm_chain.invoke({"question": question}) print(f"\n질문: {question}") print(f"답변: {response}") question_2 = "양자컴퓨팅에 대해 초등학생도 이해할 수 있게 설명해줘." response_2 = llm_chain.run(question_2) print(f"\n질문: {question_2}") print(f"답변: {response_2}") **이렇게 해서 훈련을 마치고 모델을 허깅페이스에 저장하는 방법을 알려줘.**

#### (허깅페이스에 훈련된 모델을 등록하는 순서 - 코랩에서 모델을 훈련 시켰음)

1. model.save_pretrained("tinyllama-korean-assistant")

2. tokenizer.save_pretrained("tinyllama-korean-assistant")

3. from huggingface_hub import HfApiapi = HfApi()  
api.create_repo(repo_id="tinyllama-korean-assistant", private=False)

4. model.push_to_hub("tinyllama-korean-assistant")  
tokenizer.push_to_hub("tinyllama-korean-assistant")


#### 결과(아래와 같은 이미지) - 위와 같은 설정을 하면 허깅페이스에 내가 학습시킨 모델이 등록된다. 

![[Pasted image 20250509182718.png]]