


```
import requests

from bs4 import BeautifulSoup

import logging

  

logger = logging.getLogger("news_service")

  

class NewsService:

Â  Â  def __init__(self):

Â  Â  Â  Â  pass

  

Â  Â  def get_news(self, company_name: str):

Â  Â  Â  Â  base_url = "https://search.naver.com/search.naver"

Â  Â  Â  Â  params = {

Â  Â  Â  Â  Â  Â  "where": "news",

Â  Â  Â  Â  Â  Â  "ie": "utf8",

Â  Â  Â  Â  Â  Â  "sm": "nws_hty",

Â  Â  Â  Â  Â  Â  "query": company_name

Â  Â  Â  Â  }

Â  Â  Â  Â  headers = {

Â  Â  Â  Â  Â  Â  "User-Agent": "Mozilla/5.0"

Â  Â  Â  Â  }

  

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  response = requests.get(base_url, headers=headers, params=params)

Â  Â  Â  Â  Â  Â  logger.info(f"ğŸƒâœ¨ğŸ‰ğŸŠ Response: {response.text}")

Â  Â  Â  Â  Â  Â  response.raise_for_status() Â # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ

Â  Â  Â  Â  except requests.RequestException as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ìš”ì²­ ì‹¤íŒ¨: {str(e)}")

Â  Â  Â  Â  Â  Â  return {"error": f"ë„¤ì´ë²„ ë‰´ìŠ¤ ìš”ì²­ ì‹¤íŒ¨: {str(e)}"}

  

Â  Â  Â  Â  soup = BeautifulSoup(response.text, "html.parser")

Â  Â  Â  Â  logger.info(f"ğŸƒâœ¨ğŸ‰ğŸŠ Soup: {soup}")

Â  Â  Â  Â  items = soup.select("a.n6AJosQA40hUOAe_Vplg")

Â  Â  Â  Â  logger.info(f"ğŸƒâœ¨ğŸ‰ï¿½ï¿½ Items: {items}")

  

Â  Â  Â  Â  news_list = []

Â  Â  Â  Â  for item in items[:5]: Â # ìƒìœ„ 5ê°œ ë‰´ìŠ¤ë§Œ ì¶”ì¶œ

Â  Â  Â  Â  Â  Â  title = item.get_text(strip=True)

Â  Â  Â  Â  Â  Â  link = item.get("href")

Â  Â  Â  Â  Â  Â  news_list.append({

Â  Â  Â  Â  Â  Â  Â  Â  "title": title,

Â  Â  Â  Â  Â  Â  Â  Â  "link": link

Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  logger.info(f"ğŸƒâœ¨ğŸ‰ğŸŠ News List: {news_list}")

  

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "company": company_name,

Â  Â  Â  Â  Â  Â  "news": news_list

Â  Â  Â  Â  }
```



```
  

from app.domain.service.news_service import NewsService

  
  

class NewsController:

Â  Â  def __init__(self):

Â  Â  Â  Â  self.news_service = NewsService()

  

Â  Â  def get_news(self, company_name: str):

Â  Â  Â  Â  return self.news_service.get_news(company_name)
```


ëŒ€ì‹œë³´ë“œê°€ ìˆì–´ì•¼ í•œë‹¤.

(ì¢Œì¸¡) ë„¤ë¹„ê²Œì´ì…˜ë°”ê°€ ã…‡ã…†ì–´ì•¼í•œë‹¤.



ì˜¤í”¼ìŠ¤ì ì¸ ëŠë‚Œì„ ê°•í™”í•´ì•¼ í•œë‹¤.


ì¹œí™˜ê²½ ê²½ì˜ ë¶„ì„, ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œëŸ‰ ë¶„ì„, ê¸°í›„ ì‹œë‚˜ë¦¬ì˜¤ ì‘ì„±, ì¬ë¬´ ì˜í–¥ë„ ë¶„ì„, ë¹„ì¬ë¬´ ì˜ì—­ ë¶„ì„


ë¹„ì¬ë¬´ ì˜ì—­ì—ì„œ --> ì˜¨ì‹¤ê°€ìŠ¤ ê´€ë ¨ ì¹œí™˜ê²½ ê²½ì˜ ë¶„ì„, ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œëŸ‰ ë¶„ì„, ê¸°í›„ ì‹œë‚˜ë¦¬ì˜¤, ì˜¨ì‹¤ê°€ìŠ¤ ê´€ë ¨ ì¬ë¬´ì˜í–¥ë„ ë¶„ì„


ì˜¨ì‹¤ê°€ìŠ¤ ì´ìŠˆë¡œ ì¶•ì†Œ ì‹œí‚¨ë‹¤. --> 

ê°€ìƒ íšŒì‚¬ë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤...ë°ì´í„°ì— ë”°ë¼ ì›€ì§ì—¬ì•¼ í•œë‹¤. 

ì˜¨ì‹¤ê°€ìŠ¤ ê´€ë ¨ëœ ì›Œë“œë°±ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤. 



ì˜¨ì‹¤ê°€ìŠ¤ ê´€ë ¨ëœ ê²ƒë§Œ...íŠ¹í™”ëœ ê²ƒì„ ë§Œë“¤ì....

ê°€ì§œ ê°’ì„ ë„£ì—ˆì„ ë•Œ 


ì˜¨ì‹¤ê°€ìŠ¤ ì „ë¬¸ ..... ë‰´ìŠ¤ë„ ì˜¨ì‹¤ê°€ìŠ¤ì— ê´€ë ¨ëœ.... í‚¤ì›Œë“œã…¡/////......



ìˆ˜ê¸° ë°ì´í„°(ì‚¬ëŒì´ ì˜ ê´€ë¦¬í•˜ëŠ” ê±´.. ì‚¬ëŒì´ ì§ì ‘ ì²´í¬í•˜ëŠ” ê²ƒ)ì™€ ì˜¨ë¼ì¸ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€...? 

ìˆ˜ê¸° ë°ì´í„°ì™€ ì˜¨ë¼ì¸ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ëª¨ë¸ì„ ë§Œë“¤ë ¤ê³  í•œë‹¤.................


íƒ€ì´í•‘ ì¹œê²ƒ --ê¸€ì 



```main.py
from collections import Counter

import requests

from bs4 import BeautifulSoup

import logging

  

logger = logging.getLogger("news_service")

  

class NewsService:

Â  Â  def __init__(self):

Â  Â  Â  Â  pass

  

Â  Â  def get_news(self, company_name: str):

Â  Â  Â  Â  base_url = "https://search.naver.com/search.naver"

Â  Â  Â  Â  params = {

Â  Â  Â  Â  Â  Â  "where": "news",

Â  Â  Â  Â  Â  Â  "ie": "utf8",

Â  Â  Â  Â  Â  Â  "sm": "nws_hty",

Â  Â  Â  Â  Â  Â  "query": company_name

Â  Â  Â  Â  }

Â  Â  Â  Â  headers = {

Â  Â  Â  Â  Â  Â  "User-Agent": "Mozilla/5.0"

Â  Â  Â  Â  }

  

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  response = requests.get(base_url, headers=headers, params=params)

Â  Â  Â  Â  Â  Â  logger.info(f"ğŸƒâœ¨ğŸ‰ğŸŠ Response: {response.text}")

Â  Â  Â  Â  Â  Â  response.raise_for_status() Â # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ

Â  Â  Â  Â  except requests.RequestException as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ìš”ì²­ ì‹¤íŒ¨: {str(e)}")

Â  Â  Â  Â  Â  Â  return {"error": f"ë„¤ì´ë²„ ë‰´ìŠ¤ ìš”ì²­ ì‹¤íŒ¨: {str(e)}"}

  

Â  Â  Â  Â  soup = BeautifulSoup(response.text, "html.parser")

Â  Â  Â  Â  logger.info(f"ğŸƒâœ¨ğŸ‰ğŸŠ Soup: {soup}")

Â  Â  Â  Â  items = soup.select("a.n6AJosQA40hUOAe_Vplg")

Â  Â  Â  Â  logger.info(f"ğŸƒâœ¨ğŸ‰ğŸŠ Items: {items}")

  

Â  Â  Â  Â  news_list = []

Â  Â  Â  Â  for item in items[:5]: Â # ìƒìœ„ 5ê°œ ë‰´ìŠ¤ë§Œ ì¶”ì¶œ

Â  Â  Â  Â  Â  Â  title = item.get_text(strip=True)

Â  Â  Â  Â  Â  Â  link = item.get("href")

Â  Â  Â  Â  Â  Â  news_list.append({

Â  Â  Â  Â  Â  Â  Â  Â  "title": title,

Â  Â  Â  Â  Â  Â  Â  Â  "link": link

Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  logger.info(f"ğŸƒâœ¨ğŸ‰ğŸŠ News List: {news_list}")

  

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "company": company_name,

Â  Â  Â  Â  Â  Â  "news": news_list

Â  Â  Â  Â  }

Â  Â  def get_news_content(self, url: str) -> str:

Â  Â  Â  Â  """ê° ë‰´ìŠ¤ ë§í¬ì—ì„œ ë³¸ë¬¸ í¬ë¡¤ë§"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})

Â  Â  Â  Â  Â  Â  response.raise_for_status()

Â  Â  Â  Â  Â  Â  soup = BeautifulSoup(response.text, "html.parser")

  

Â  Â  Â  Â  Â  Â  # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ìœ„ì¹˜ (ê³µí†µì ìœ¼ë¡œ ë§ì´ ì“°ì´ëŠ” ì˜ì—­)

Â  Â  Â  Â  Â  Â  article = soup.select_one('div#dic_area') Â # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ID

Â  Â  Â  Â  Â  Â  if not article:

Â  Â  Â  Â  Â  Â  Â  Â  article = soup.find('article') Â # ì¼ë¶€ ë‹¤ë¥¸ í¬í„¸ ê¸°ì‚¬ ëŒ€ì‘

Â  Â  Â  Â  Â  Â  content = article.get_text(strip=True) if article else "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

Â  Â  Â  Â  Â  Â  return content

  

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")

Â  Â  Â  Â  Â  Â  return "ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨"

Â  Â  def analyze_esg_keywords(self, contents: list) -> dict:

Â  Â  Â  Â  """ë‰´ìŠ¤ ë³¸ë¬¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ ESG í‚¤ì›Œë“œ ë“±ì¥ ë¹ˆë„ ë¶„ì„"""

Â  Â  Â  Â  # txt íŒŒì¼ì—ì„œ ESG í‚¤ì›Œë“œ ë¶ˆëŸ¬ì˜¤ê¸°

Â  Â  Â  Â  with open("app/domain/service/esg_keywords.txt", "r", encoding="utf-8") as file:

Â  Â  Â  Â  Â  Â  esg_keywords = [line.strip() for line in file.readlines() if line.strip()]

Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ” ESG í‚¤ì›Œë“œ ë¶ˆëŸ¬ì˜¤ê¸° ì„±ê³µ: {len(esg_keywords)}ê°œ í‚¤ì›Œë“œ")

  

Â  Â  Â  Â  combined_text = " ".join(contents)

Â  Â  Â  Â  word_freq = Counter()

  

Â  Â  Â  Â  for word in esg_keywords:

Â  Â  Â  Â  Â  Â  count = combined_text.count(word)

Â  Â  Â  Â  Â  Â  if count > 0:

Â  Â  Â  Â  Â  Â  Â  Â  word_freq[word] = count

  

Â  Â  Â  Â  return dict(word_freq)
```



```news_controller.py
from app.domain.service.news_service import NewsService

  
  

class NewsController:

Â  Â  def __init__(self):

Â  Â  Â  Â  self.news_service = NewsService()

  

Â  Â  def get_news(self, company_name: str):

Â  Â  Â  Â  news_data = self.news_service.get_news(company_name)

Â  Â  Â  Â  news_list = news_data["news"] Â # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ

  

Â  Â  Â  Â  # ë³¸ë¬¸ë§Œ ì¶”ì¶œí•´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±

Â  Â  Â  Â  contents = [item["content"] for item in news_list if "content" in item]

  

Â  Â  Â  Â  # ESG í‚¤ì›Œë“œ ë¶„ì„

Â  Â  Â  Â  esg_analysis = self.news_service.analyze_esg_keywords(contents)

  

Â  Â  Â  Â  # ESG ë¶„ì„ ê²°ê³¼ í¬í•¨í•œ ì‘ë‹µ ìƒì„±

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "company": company_name,

Â  Â  Â  Â  Â  Â  "news": news_list,

Â  Â  Â  Â  Â  Â  "esg_analysis": esg_analysis

Â  Â  Â  Â  }
```


```news_router.py
from fastapi import APIRouter,Request

from fastapi.responses import JSONResponse

import logging

from app.domain.controlloer.news_controller import NewsController

from app.domain.model.news_schema import NewsRequest

  

router = APIRouter()

logger = logging.getLogger("news_main")

news_controller = NewsController()

  

@router.post("/search")

async def news(req: NewsRequest):

Â  Â  logger.info(f"ğŸ” ê¸°ì—…ëª… ìˆ˜ì‹ : {req.company_name}")

Â  Â  result = news_controller.get_news(req.company_name)

Â  Â  return JSONResponse(content=result)
```

```neww_schema.py
from pydantic import BaseModel

  

class NewsRequest(BaseModel):

Â  Â  company_name: str
```


```DockerFile
# Python 3.12.7-slim ì´ë¯¸ì§€ ì‚¬ìš©

FROM python:3.12.7-slim

  

WORKDIR /app

  

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜ (ê¸°ì¡´ ëª©ë¡ì— build-essential ì¶”ê°€)

# build-essentialì€ C/C++ í™•ì¥ ëª¨ë“ˆ ì»´íŒŒì¼ì— í•„ìš”í•œ ë„êµ¬ ëª¨ìŒì…ë‹ˆë‹¤.

RUN apt-get update && apt-get install -y \

Â  Â  gcc \

Â  Â  g++ \

Â  Â  make \

Â  Â  libpq-dev \

Â  Â  curl \

Â  Â  wget \

Â  Â  git \

Â  Â  fontconfig \

Â  Â  fonts-nanum \

Â  Â  build-essential \

Â && fc-cache -fv \

Â && rm -rf /var/lib/apt/lists/*

  

# 1ë‹¨ê³„: pipë¥¼ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ

# python -m pipë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ Docker ì´ë¯¸ì§€ì˜ Python í™˜ê²½ì— ë§ëŠ” pipë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

RUN python -m pip install --upgrade pip

  

# 2ë‹¨ê³„: ì—…ê·¸ë ˆì´ë“œëœ pipë¥¼ ì‚¬ìš©í•˜ì—¬ setuptoolsì™€ wheelì„ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜/ì—…ê·¸ë ˆì´ë“œ

# Python 3.12ì—ì„œëŠ” ìµœì‹  setuptoolsê°€ distutilsë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë¯€ë¡œ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

RUN python -m pip install --upgrade setuptools wheel

  

# 3ë‹¨ê³„: requirements.txt ë³µì‚¬ ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜

COPY requirements.txt .

# --no-cache-dir ì˜µì…˜ì€ Docker ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

# --verbose ì˜µì…˜ì„ ì¶”ê°€í•˜ì—¬ ì„¤ì¹˜ ê³¼ì •ì— ëŒ€í•œ ìì„¸í•œ ë¡œê·¸ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë¬¸ì œ ë°œìƒ ì‹œ ë””ë²„ê¹…ì— ìœ ìš©).

RUN python -m pip install --no-cache-dir -r requirements.txt --verbose

  

COPY . .

  

EXPOSE 8003

  

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8003", "--reload"]
```

```requirements.txt
fastapi==0.110.0

uvicorn==0.27.1

pydantic==2.6.3

python-dotenv==1.0.1

requests==2.31.0

beautifulsoup4==4.12.3

aiohttp==3.9.3

uvloop==0.19.0

httptools==0.6.1

sqlalchemy==2.0.27

alembic==1.13.1

psycopg2-binary==2.9.9

asyncpg==0.29.0

email_validator Â # ë˜ëŠ” email-validator==íŠ¹ì •ë²„ì „

passlib[bcrypt]==1.7.4 # ìµœì‹  ë²„ì „ ê²€í†  ê¶Œì¥

shortuuid==1.0.13

python-jose[cryptography] # ë˜ëŠ” python-jose[cryptography]==íŠ¹ì •ë²„ì „

redis==5.0.4 # í‘œì¤€ redis-py ì‚¬ìš© ì‹œ

httpx==0.27.0

wordcloud==1.9.4

matplotlib~=3.8.4 # Python 3.12 í˜¸í™˜ ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸

numpy~=1.26.4 Â  Â # Python 3.12 í˜¸í™˜ ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (ë˜ëŠ” ~=2.0.1)

pandas~=2.2.2 Â  Â  # Python 3.12 í˜¸í™˜ ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸
```



í´ë¦­ ì´ë²¤íŠ¸ê°€ ë°œìƒë˜ëŠ”ê±°ì•¼? 