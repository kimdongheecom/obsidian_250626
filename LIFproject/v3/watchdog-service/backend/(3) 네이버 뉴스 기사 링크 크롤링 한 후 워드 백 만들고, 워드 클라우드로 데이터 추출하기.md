


```
import requests

from bs4 import BeautifulSoup

import logging

  

logger = logging.getLogger("news_service")

  

class NewsService:

    def __init__(self):

        pass

  

    def get_news(self, company_name: str):

        base_url = "https://search.naver.com/search.naver"

        params = {

            "where": "news",

            "ie": "utf8",

            "sm": "nws_hty",

            "query": company_name

        }

        headers = {

            "User-Agent": "Mozilla/5.0"

        }

  

        try:

            response = requests.get(base_url, headers=headers, params=params)

            logger.info(f"🎃✨🎉🎊 Response: {response.text}")

            response.raise_for_status()  # HTTP 오류 발생 시 예외 발생

        except requests.RequestException as e:

            logger.error(f"❌ 네이버 뉴스 요청 실패: {str(e)}")

            return {"error": f"네이버 뉴스 요청 실패: {str(e)}"}

  

        soup = BeautifulSoup(response.text, "html.parser")

        logger.info(f"🎃✨🎉🎊 Soup: {soup}")

        items = soup.select("a.n6AJosQA40hUOAe_Vplg")

        logger.info(f"🎃✨🎉�� Items: {items}")

  

        news_list = []

        for item in items[:5]:  # 상위 5개 뉴스만 추출

            title = item.get_text(strip=True)

            link = item.get("href")

            news_list.append({

                "title": title,

                "link": link

            })

        logger.info(f"🎃✨🎉🎊 News List: {news_list}")

  

        return {

            "company": company_name,

            "news": news_list

        }
```



```
  

from app.domain.service.news_service import NewsService

  
  

class NewsController:

    def __init__(self):

        self.news_service = NewsService()

  

    def get_news(self, company_name: str):

        return self.news_service.get_news(company_name)
```


대시보드가 있어야 한다.

(좌측) 네비게이션바가 ㅇㅆ어야한다.



오피스적인 느낌을 강화해야 한다.


친환경 경영 분석, 온실가스 배출량 분석, 기후 시나리오 작성, 재무 영향도 분석, 비재무 영역 분석


비재무 영역에서 --> 온실가스 관련 친환경 경영 분석, 온실가스 배출량 분석, 기후 시나리오, 온실가스 관련 재무영향도 분석


온실가스 이슈로 축소 시킨다. --> 

가상 회사를 만들어야 한다...데이터에 따라 움직여야 한다. 

온실가스 관련된 워드백을 만들어야 한다. 



온실가스 관련된 것만...특화된 것을 만들자....

가짜 값을 넣었을 때 


온실가스 전문 ..... 뉴스도 온실가스에 관련된.... 키워드ㅡ/////......



수기 데이터(사람이 잘 관리하는 건.. 사람이 직접 체크하는 것)와 온라인 데이터를 어떻게 처리할지...? 

수기 데이터와 온라인 데이터를 가지고 모델을 만들려고 한다.................


타이핑 친것 --글자 



```main.py
from collections import Counter

import requests

from bs4 import BeautifulSoup

import logging

  

logger = logging.getLogger("news_service")

  

class NewsService:

    def __init__(self):

        pass

  

    def get_news(self, company_name: str):

        base_url = "https://search.naver.com/search.naver"

        params = {

            "where": "news",

            "ie": "utf8",

            "sm": "nws_hty",

            "query": company_name

        }

        headers = {

            "User-Agent": "Mozilla/5.0"

        }

  

        try:

            response = requests.get(base_url, headers=headers, params=params)

            logger.info(f"🎃✨🎉🎊 Response: {response.text}")

            response.raise_for_status()  # HTTP 오류 발생 시 예외 발생

        except requests.RequestException as e:

            logger.error(f"❌ 네이버 뉴스 요청 실패: {str(e)}")

            return {"error": f"네이버 뉴스 요청 실패: {str(e)}"}

  

        soup = BeautifulSoup(response.text, "html.parser")

        logger.info(f"🎃✨🎉🎊 Soup: {soup}")

        items = soup.select("a.n6AJosQA40hUOAe_Vplg")

        logger.info(f"🎃✨🎉🎊 Items: {items}")

  

        news_list = []

        for item in items[:5]:  # 상위 5개 뉴스만 추출

            title = item.get_text(strip=True)

            link = item.get("href")

            news_list.append({

                "title": title,

                "link": link

            })

        logger.info(f"🎃✨🎉🎊 News List: {news_list}")

  

        return {

            "company": company_name,

            "news": news_list

        }

    def get_news_content(self, url: str) -> str:

        """각 뉴스 링크에서 본문 크롤링"""

        try:

            response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})

            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")

  

            # 네이버 뉴스 본문 위치 (공통적으로 많이 쓰이는 영역)

            article = soup.select_one('div#dic_area')  # 네이버 뉴스 본문 ID

            if not article:

                article = soup.find('article')  # 일부 다른 포털 기사 대응

            content = article.get_text(strip=True) if article else "본문 추출 실패"

            return content

  

        except Exception as e:

            logger.error(f"❌ 뉴스 본문 크롤링 실패: {str(e)}")

            return "본문 크롤링 실패"

    def analyze_esg_keywords(self, contents: list) -> dict:

        """뉴스 본문 리스트에서 ESG 키워드 등장 빈도 분석"""

        # txt 파일에서 ESG 키워드 불러오기

        with open("app/domain/service/esg_keywords.txt", "r", encoding="utf-8") as file:

            esg_keywords = [line.strip() for line in file.readlines() if line.strip()]

            logger.info(f"🔍 ESG 키워드 불러오기 성공: {len(esg_keywords)}개 키워드")

  

        combined_text = " ".join(contents)

        word_freq = Counter()

  

        for word in esg_keywords:

            count = combined_text.count(word)

            if count > 0:

                word_freq[word] = count

  

        return dict(word_freq)
```



```news_controller.py
from app.domain.service.news_service import NewsService

  
  

class NewsController:

    def __init__(self):

        self.news_service = NewsService()

  

    def get_news(self, company_name: str):

        news_data = self.news_service.get_news(company_name)

        news_list = news_data["news"]  # 뉴스 리스트 추출

  

        # 본문만 추출해서 리스트로 구성

        contents = [item["content"] for item in news_list if "content" in item]

  

        # ESG 키워드 분석

        esg_analysis = self.news_service.analyze_esg_keywords(contents)

  

        # ESG 분석 결과 포함한 응답 생성

        return {

            "company": company_name,

            "news": news_list,

            "esg_analysis": esg_analysis

        }
```


```news_router.py
from fastapi import APIRouter,Request

from fastapi.responses import JSONResponse

import logging

from app.domain.controlloer.news_controller import NewsController

from app.domain.model.news_schema import NewsRequest

  

router = APIRouter()

logger = logging.getLogger("news_main")

news_controller = NewsController()

  

@router.post("/search")

async def news(req: NewsRequest):

    logger.info(f"🔍 기업명 수신: {req.company_name}")

    result = news_controller.get_news(req.company_name)

    return JSONResponse(content=result)
```

```neww_schema.py
from pydantic import BaseModel

  

class NewsRequest(BaseModel):

    company_name: str
```


```DockerFile
# Python 3.12.7-slim 이미지 사용

FROM python:3.12.7-slim

  

WORKDIR /app

  

# 시스템 의존성 설치 (기존 목록에 build-essential 추가)

# build-essential은 C/C++ 확장 모듈 컴파일에 필요한 도구 모음입니다.

RUN apt-get update && apt-get install -y \

    gcc \

    g++ \

    make \

    libpq-dev \

    curl \

    wget \

    git \

    fontconfig \

    fonts-nanum \

    build-essential \

 && fc-cache -fv \

 && rm -rf /var/lib/apt/lists/*

  

# 1단계: pip를 최신 버전으로 업그레이드

# python -m pip를 사용하여 현재 Docker 이미지의 Python 환경에 맞는 pip를 명시적으로 실행합니다.

RUN python -m pip install --upgrade pip

  

# 2단계: 업그레이드된 pip를 사용하여 setuptools와 wheel을 최신 버전으로 설치/업그레이드

# Python 3.12에서는 최신 setuptools가 distutils를 포함하고 있으므로 매우 중요합니다.

RUN python -m pip install --upgrade setuptools wheel

  

# 3단계: requirements.txt 복사 및 패키지 설치

COPY requirements.txt .

# --no-cache-dir 옵션은 Docker 이미지 크기를 줄이는 데 도움이 됩니다.

# --verbose 옵션을 추가하여 설치 과정에 대한 자세한 로그를 볼 수 있습니다 (문제 발생 시 디버깅에 유용).

RUN python -m pip install --no-cache-dir -r requirements.txt --verbose

  

COPY . .

  

EXPOSE 8003

  

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8003", "--reload"]
```

```requirements.txt
fastapi==0.110.0

uvicorn==0.27.1

pydantic==2.6.3

python-dotenv==1.0.1

requests==2.31.0

beautifulsoup4==4.12.3

aiohttp==3.9.3

uvloop==0.19.0

httptools==0.6.1

sqlalchemy==2.0.27

alembic==1.13.1

psycopg2-binary==2.9.9

asyncpg==0.29.0

email_validator  # 또는 email-validator==특정버전

passlib[bcrypt]==1.7.4 # 최신 버전 검토 권장

shortuuid==1.0.13

python-jose[cryptography] # 또는 python-jose[cryptography]==특정버전

redis==5.0.4 # 표준 redis-py 사용 시

httpx==0.27.0

wordcloud==1.9.4

matplotlib~=3.8.4 # Python 3.12 호환 버전으로 업데이트

numpy~=1.26.4    # Python 3.12 호환 버전으로 업데이트 (또는 ~=2.0.1)

pandas~=2.2.2     # Python 3.12 호환 버전으로 업데이트
```



클릭 이벤트가 발생되는거야? 