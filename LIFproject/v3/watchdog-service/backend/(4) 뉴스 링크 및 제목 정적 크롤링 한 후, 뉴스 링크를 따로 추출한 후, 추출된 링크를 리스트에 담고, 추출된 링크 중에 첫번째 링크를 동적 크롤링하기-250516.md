
#### news_service.py 코드를 바탕으로 동적 크롤링과 워드클라우드 이미지 생성 및 저장 과정을 단계별로 설명드리겠습니다. 이 코드는 웹에서 뉴스 정보를 가져와 분석하고 시각화하는 일반적인 파이프라인을 잘 보여주고 있습니다.

**코드의 전체적인 흐름:**

1. **뉴스 목록 수집 (정적 크롤링):** 특정 회사명으로 네이버 뉴스에서 관련 뉴스 기사들의 제목과 링크를 가져옵니다. (get_news 함수 일부)
    
2. **뉴스 본문 수집 (동적 크롤링):** 수집된 뉴스 링크 중 일부(여기서는 최대 5개)에 접속하여 각 기사의 본문 내용을 Selenium을 사용하여 가져옵니다. (crawl_with_selenium 함수)
    
3. **텍스트 전처리 및 자연어 처리(NLP):** 크롤링한 본문 텍스트에서 명사를 추출하고 불용어를 제거하여 핵심 단어들의 빈도수를 계산합니다. (process_text_for_nlp 함수)
    
4. **워드클라우드 생성 및 저장:** 계산된 단어 빈도수를 기반으로 워드클라우드 이미지를 생성하고, 지정된 폴더에 이미지 파일로 저장합니다. (generate_wordcloud_image_from_freq 함수)
    

이제 각 단계를 코드와 함께 자세히 살펴보겠습니다.

---

**단계 1: 서비스 초기화 (__init__ 함수)**

```
OUTPUT_DIR = 'app/static/output'

class NewsService:
    def __init__(self):
        # Okt 형태소 분석기 초기화
        try:
            self.okt = Okt()
            logger.info("✅ Okt 형태소 분석기 초기화 성공")
        except Exception as e:
            logger.error(f"❌ Okt 형태소 분석기 초기화 실패: {e}. NLP 기능이 제한될 수 있습니다.")
            self.okt = None
        
        # 출력 디렉터리 생성
        try:
            if not os.path.exists(OUTPUT_DIR):
                os.makedirs(OUTPUT_DIR)
                logger.info(f"✅ 출력 디렉터리 '{OUTPUT_DIR}'가 생성되었습니다.")
        except OSError as e:
            logger.error(f"❌ 출력 디렉터리 '{OUTPUT_DIR}' 생성 실패: {e}")
```

- **Okt 형태소 분석기 초기화:**
    
    - self.okt = Okt(): KoNLPy 라이브러리의 Okt 형태소 분석기 객체를 생성하여 self.okt 인스턴스 변수에 저장합니다. Okt는 한국어 텍스트에서 명사, 동사 등의 품사를 태깅하고 형태소를 분석하는 데 사용됩니다.
        
    - 이 작업은 초기화 시 한 번만 수행되어, 이후 텍스트 처리 요청 시마다 객체를 새로 생성하는 오버헤드를 줄입니다.
        
    - try-except 블록으로 감싸서 초기화 중 발생할 수 있는 오류(예: Java 경로 문제)를 처리하고 로깅합니다.
        
- **출력 디렉터리 생성:**
    
    - OUTPUT_DIR = 'app/static/output': 워드클라우드 이미지를 저장할 경로를 정의합니다.
        
    - os.path.exists(OUTPUT_DIR): 해당 경로에 디렉터리가 이미 존재하는지 확인합니다.
        
    - os.makedirs(OUTPUT_DIR): 디렉터리가 없다면 생성합니다. makedirs는 중간 경로의 디렉터리도 함께 생성해 줍니다.
        
    - 이 역시 서비스 시작 시 한 번만 실행되어, 이미지를 저장할 때마다 디렉터리 존재 여부를 확인하고 생성하는 반복적인 작업을 피합니다.
        

---

**단계 2: 뉴스 검색 및 링크 수집 (get_news 함수 일부)**
```
    def get_news(self, company_name: str):
        # ... (네이버 뉴스 검색을 위한 URL, 파라미터, 헤더 설정) ...
        try:
            response = requests.get(base_url, headers=headers, params=params)
            response.raise_for_status() 
        except requests.RequestException as e:
            # ... (오류 처리) ...

        soup = BeautifulSoup(response.text, "html.parser")
        items = soup.select("span[class='sds-comps-text sds-comps-text-ellipsis-1 sds-comps-text-type-headline1']")
        
        news_list = []
        for item in items[:5]: # 상위 5개 뉴스만
            title = item.get_text(strip=True)
            # ... (a 태그에서 링크 추출 로직) ...
            news_list.append({"title": title, "link": link})
        
        links = [news['link'] for news in news_list if news.get('link')]
        # ... (이후 로직으로 연결) ...
```

- **HTTP 요청:**
    
    - requests.get(): requests 라이브러리를 사용하여 네이버 뉴스 검색 결과 페이지에 GET 요청을 보냅니다. company_name이 검색어가 됩니다.
        
    - response.raise_for_status(): HTTP 요청이 실패하면 (상태 코드가 4xx 또는 5xx 이면) 예외를 발생시켜 오류 처리를 합니다.
        
- **HTML 파싱:**
    
    - BeautifulSoup(response.text, "html.parser"): 받아온 HTML 응답 텍스트를 BeautifulSoup 객체로 변환하여 HTML 구조를 쉽게 탐색하고 원하는 데이터를 추출할 수 있도록 합니다.
        
- **뉴스 아이템 선택 및 정보 추출:**
    
    - soup.select(...): CSS 선택자를 사용하여 뉴스 제목을 담고 있는 span 태그들을 찾습니다.
        
    - 반복문을 통해 상위 5개의 뉴스 아이템에 대해 제목(item.get_text(strip=True))과 해당 뉴스로 연결되는 링크(parent_element.get('href'))를 추출합니다. 링크는 제목을 포함하는 span 태그의 부모 a 태그에서 가져옵니다.
        
    - 추출된 제목과 링크는 news_list (딕셔너리 리스트)에 저장됩니다.
        
- **링크 리스트 생성:**
    
    - links = [news['link'] for news in news_list if news.get('link')]: 유효한 링크만 따로 리스트로 만듭니다.
        

---

**단계 3: 뉴스 본문 동적 크롤링 (get_news 함수 내 호출 및 crawl_with_selenium 함수)**

```
    # get_news 함수 내:
    # ...
    for i, link in enumerate(links[:5], start=1): # 최대 5개의 링크에 대해 반복
        content = self.crawl_with_selenium(link)    # 각 링크의 본문 크롤링
        word_freq = self.process_text_for_nlp(content) # 크롤링된 내용으로 NLP 처리
        self.generate_wordcloud_image_from_freq(word_freq, i) # 워드클라우드 생성 및 저장
    # ...

    def crawl_with_selenium(self, link: str) -> str:
        CHROMEDRIVER_IN_CONTAINER_PATH = "/usr/bin/chromedriver" 
        options = Options()
        # ... (headless, no-sandbox 등 Chrome 옵션 설정) ...
        service = ChromeService(executable_path=CHROMEDRIVER_IN_CONTAINER_PATH)
        driver = None
        try:
            driver = webdriver.Chrome(service=service, options=options)
            driver.get(link) # 뉴스 기사 페이지 로드
            driver.implicitly_wait(5) # 페이지 요소 로딩 대기

            selectors = [ # 본문 내용을 찾기 위한 다양한 CSS 선택자 목록
                # ... (선택자들) ...
            ]
            content_text = ""
            for i, selector in enumerate(selectors): # 각 선택자 시도
                try:
                    elements = driver.find_elements("css selector", selector)
                    if elements:
                        # ... (가장 긴 텍스트를 가진 요소를 본문으로 선택하는 로직 - 이전 답변에서 추가 제안) ...
                        # 현재 코드: 첫 번째 비어있지 않은 요소의 텍스트 사용
                        for elem in elements:
                            elem_text = elem.text.strip()
                            if elem_text:
                                content_text = elem_text
                                break
                        if content_text:
                            break 
                except Exception as e_select:
                    # ... (선택자 오류 로깅) ...
            
            if not content_text: # 모든 선택자로 못 찾으면 body 전체 텍스트 시도
                # ... (body 텍스트 추출 또는 오류 메시지 반환) ...
            return content_text
        except Exception as e:
            # ... (Selenium 오류 처리) ...
            return f"[Selenium 크롤링 오류]: {str(e)}"
        finally:
            if driver:
                driver.quit() # 브라우저 종료
```


- **Selenium WebDriver 설정:**
    
    - Options(): Chrome 브라우저 실행 옵션을 설정합니다.
        
        - --headless: GUI 없이 백그라운드에서 실행 (서버 환경 필수).
            
        - --no-sandbox, --disable-dev-shm-usage: Docker 환경에서 안정적인 실행을 위한 일반적인 옵션.
            
    - Service(): ChromeDriver 실행 파일의 경로를 지정합니다.
        
    - webdriver.Chrome(): 설정된 옵션과 서비스로 Chrome WebDriver 인스턴스를 생성합니다.
        
- **페이지 로드 및 대기:**
    
    - driver.get(link): WebDriver가 주어진 link의 웹 페이지를 엽니다.
        
    - driver.implicitly_wait(5): 페이지의 요소들이 로드될 때까지 최대 5초간 암묵적으로 대기합니다. JavaScript 등으로 동적으로 콘텐츠가 로드되는 경우 필요합니다.
        
- **본문 내용 추출:**
    
    - selectors 리스트: 뉴스 기사 본문이 위치할 가능성이 있는 다양한 CSS 선택자들을 정의합니다. 웹사이트마다 HTML 구조가 다르기 때문에 여러 선택자를 시도합니다.
        
    - driver.find_elements("css selector", selector): 각 선택자에 해당하는 웹 요소들을 찾습니다.
        
    - 요소를 찾으면, 그 요소의 텍스트(elem.text.strip())를 가져와 content_text에 저장합니다. (현재 코드는 처음 찾은 비어있지 않은 요소의 텍스트를 사용합니다. 이전 답변에서는 가장 긴 텍스트를 가진 요소를 선택하는 개선안을 제안했습니다.)
        
    - 만약 정의된 선택자들로 본문을 찾지 못하면, 페이지의 body 전체 텍스트를 가져오는 것을 시도합니다.
        
- **WebDriver 종료:**
    
    - finally 블록 안에서 driver.quit(): 크롤링 작업이 성공하든 실패하든 관계없이 WebDriver와 브라우저 프로세스를 확실하게 종료하여 리소스 누수를 방지합니다.
        

---

**단계 4: 텍스트 전처리 및 자연어 처리 (NLP) (process_text_for_nlp 함수)**

```
    def process_text_for_nlp(self, text: str, custom_stopwords: list = None) -> Counter:
        if not self.okt: # Okt 초기화 실패 시
            return Counter()
        if not text or text.startswith("["): # 유효하지 않은 텍스트 입력 시
            return Counter()

        # 1. 텍스트 정제: 한글, 영문, 숫자, 공백 외 특수문자 제거
        processed_text = re.sub(r'[^가-힣A-Za-z0-9\s]', '', text)

        # 2. 명사 추출 (Okt 사용)
        try:
            nouns = self.okt.nouns(processed_text)
        except Exception as e:
            return Counter()

        # 3. 불용어 처리
        default_stopwords_set = set([...]) # 기본 불용어 목록
        final_stopwords = default_stopwords_set
        if custom_stopwords:
            final_stopwords.update(custom_stopwords)
        
        meaningful_nouns = [ # 두 글자 이상이고 불용어가 아닌 명사만 선택
            noun for noun in nouns if len(noun) > 1 and noun.lower() not in final_stopwords
        ]

        if not meaningful_nouns:
            return Counter()
            
        # 4. 단어 빈도수 계산
        word_freq = Counter(meaningful_nouns)
        return word_freq
```

- **입력 유효성 검사:** Okt 객체가 제대로 초기화되었는지, 입력 텍스트가 유효한지 확인합니다.
    
- **텍스트 정제:** re.sub()를 사용하여 정규표현식으로 한글, 영문 알파벳, 숫자, 공백을 제외한 모든 특수문자를 제거합니다. 이는 분석에 불필요한 노이즈를 줄입니다.
    
- **명사 추출:** self.okt.nouns(processed_text)를 호출하여 정제된 텍스트에서 명사만 추출합니다. 워드클라우드에서는 보통 명사가 핵심 키워드로 사용됩니다.
    
- **불용어(Stopwords) 처리:**
    
    - default_stopwords_set: 분석에 의미가 없거나 너무 흔하게 등장하여 중요도가 낮은 단어들(예: "기자", "뉴스", 조사, 어미 등)의 목록을 정의합니다.
        
    - custom_stopwords: 함수 호출 시 추가적인 불용어 목록을 전달받을 수 있도록 합니다.
        
    - 추출된 명사 중에서 길이가 한 글자인 단어나 불용어 목록에 포함된 단어들을 제거하여 meaningful_nouns 리스트를 만듭니다.
        
- **단어 빈도수 계산:** collections.Counter(meaningful_nouns)를 사용하여 의미 있는 명사들의 등장 빈도수를 계산합니다. Counter 객체는 딕셔너리 형태로 각 단어와 그 빈도수를 저장합니다.
    

---

**단계 5: 워드클라우드 생성 및 이미지 저장 (generate_wordcloud_image_from_freq 함수)**

```
    def generate_wordcloud_image_from_freq(self, word_freq: Counter, num: int = 1, font_path: str = FONT_PATH) -> str:
        if not isinstance(word_freq, Counter) or not word_freq: # 유효한 입력 데이터 확인
            return "" 

        output_dir = OUTPUT_DIR 
        output_filename = f"news_cloud_{num}.png" # 파일명에 순번(num) 포함
        output_path = os.path.join(output_dir, output_filename)

        try: # 디렉터리 생성은 __init__에서 하므로 여기서는 생략 가능, 또는 방어적으로 추가
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
        except OSError as e:
            return "" 

        try:
            wc = WordCloud(
                font_path=font_path, # 한글 폰트 경로
                width=800,
                height=400,
                background_color="white",
                max_words=100, # 표시할 최대 단어 수
            ).generate_from_frequencies(dict(word_freq)) # Counter 객체를 dict로 변환하여 전달

            wc.to_file(output_path) # 이미지 파일로 저장
            logger.info(f"🖼️ 워드클라우드 이미지가 {output_path}에 저장되었습니다.")
            return output_path # 저장된 파일 경로 반환
        except Exception as e:
            # ... (오류 처리 및 로깅) ...
            return ""
```

- **입력 유효성 검사:** word_freq가 비어있거나 유효한 Counter 객체가 아니면 빈 문자열을 반환합니다.
    
- **출력 경로 설정:**
    
    - output_filename = f"news_cloud_{num}.png": 저장될 이미지 파일명에 num (뉴스 기사의 순번)을 포함시켜 각 기사별 워드클라우드를 구분하여 저장할 수 있도록 합니다.
        
    - os.path.join(output_dir, output_filename): OS에 독립적인 방식으로 전체 파일 경로를 생성합니다.
        
- **WordCloud 객체 생성 및 이미지 생성:**
    
    - WordCloud(...): wordcloud 라이브러리의 WordCloud 객체를 생성합니다.
        
        - font_path=font_path: 한글을 제대로 표시하기 위해 한글 폰트 파일의 경로를 지정합니다.
            
        - width, height, background_color, max_words: 워드클라우드의 크기, 배경색, 표시할 최대 단어 수 등을 설정합니다.
            
    - .generate_from_frequencies(dict(word_freq)): Counter 객체를 딕셔너리 형태로 변환하여 전달하면, 단어 빈도수에 따라 크기가 다른 단어들로 워드클라우드를 생성합니다.
        
- **이미지 파일 저장:**
    
    - wc.to_file(output_path): 생성된 워드클라우드 이미지를 지정된 output_path에 PNG 파일로 저장합니다.
        
- **결과 반환:** 저장된 이미지 파일의 경로를 문자열로 반환합니다.
    

---

**요약 및 학습 포인트:**

- **웹 크롤링:**
    
    - **정적 크롤링 (requests, BeautifulSoup):** 초기 정보 수집(뉴스 목록)에 빠르고 효율적입니다.
        
    - **동적 크롤링 (Selenium):** JavaScript로 동적으로 로드되는 콘텐츠(뉴스 본문)를 가져오는 데 필수적입니다. Headless 모드, Docker 환경 설정, 다양한 선택자 사용, WebDriver 생명주기 관리가 중요합니다.
        
- **자연어 처리 (NLP):**
    
    - **형태소 분석 (KoNLPy Okt):** 한국어 텍스트에서 의미 단위(명사 등)를 추출하는 기본 단계입니다.
        
    - **텍스트 정제 (re):** 분석에 불필요한 특수문자 등을 제거하여 데이터의 질을 높입니다.
        
    - **불용어 처리:** 분석 결과의 정확도와 가독성을 높이기 위해 의미 없는 단어를 제거합니다.
        
    - **단어 빈도 분석 (collections.Counter):** 텍스트 내 주요 키워드를 파악하는 데 유용합니다.
        
- **데이터 시각화:**
    
    - **워드클라우드 (wordcloud):** 단어 빈도수를 시각적으로 표현하여 직관적인 이해를 돕습니다. 한글 폰트 설정이 중요합니다.
        
    - **이미지 저장 (WordCloud.to_file(), os 모듈):** 생성된 시각화 결과를 파일로 저장하고 관리합니다. 디렉터리 관리(os.makedirs)가 필요할 수 있습니다.
        
- **오류 처리 및 로깅:**
    
    - try-except 블록을 사용하여 네트워크 오류, 파일 처리 오류, 라이브러리 오류 등 다양한 예외 상황을 적절히 처리합니다.
        
    - logging 모듈을 사용하여 각 단계의 진행 상황과 오류 정보를 기록하여 디버깅 및 모니터링에 활용합니다.
        
- **코드 구조 및 객체 지향:**
    
    - NewsService 클래스로 관련 기능들을 묶어 관리합니다.
        
    - __init__에서 리소스(Okt 객체, 디렉터리)를 초기화하여 효율성을 높입니다.
        

이 코드는 웹 데이터 수집부터 분석, 시각화까지 이어지는 실용적인 예제이며, 각 라이브러리의 기본적인 사용법과 주의사항을 학습하는 데 도움이 될 것입니다. 각 함수의 역할과 데이터 흐름을 이해하는 것이 중요합니다.