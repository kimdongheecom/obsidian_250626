- html 기반 크롤링 하기(네이버 금융 사이트에서 html 기반 크롤링 하는 방법)
- `requests + BeautifulSoup` 기반으로 여러 종목을 **단순한 HTML 파싱**만으로 처리
- **Pydantic v2 기준** + **PostgreSQL + FastAPI + DDD 구조 + Swagger 대응**을 모두 반영해서  삼성전자 주가 크롤링 → DB 저장 → JSON 응답` 흐름으로 전체 코드를 **최신 스타일로 다시 구현**
- stocks 테이블을 생성해야 한다. 그래야 DB와 연결되어 있는지 알 수 있다.
- 이 프로젝트는 FastAPI 기반 비동기 API 서비스이며,  SQLAlchemy + asyncpg 조합으로 PostgreSQL과 비동기적으로 연결됩니다.


**1. 기본 구조 프롬프팅, 커서에게 요청,**
stock/  
├── app/  
│ ├── __init__.py  
│ ├── main.py  
├── .env  
├── requirements.txt  
├── Dockerfile  
├── docker-compose.yml  
└── README.md  
  
**이 구조로 만들어주고 docker-compose.yml을 통해 fastapi를 실행할 수 있도록 해줘. 도커file에서 파이썬버전은 3.12.7이고 reload옵션을 추가해줬으면 해. 그다음에 port는 9000으로 해줘. 도메인 주소는 www.kimdonghee.com 이야. APIRouter 생성자의 prefix는 /stock으로 하고, async방식으로 작동되게 해줘. requirements.txt에 기본적으로 등록된 라이브러리는 다음과 같다.** 
**fastapi uvicorn asyncpg sqlalchemy python-dotenv pytz email_validator passlib[bcrypt]==1.7.4 shortuuid==1.0.13 python-jose[cryptography] redis==5.2.1 그리고 최종적으로 swagger에서 ("/")하면 Welcome Message가 뜰 수 있게 해줘. app.include_router(stock_router)처럼 app이 stock_router를 포함하게 해줘.**

**그리고 docker-compose.yml을 만들 때, 아래와 같은 형식을 붙이고, 내 코드에 맞게 변환해줘. (도커 컨테이너에 올릴 백엔드 컨테이너, DB 컨테이너 생성하기)**

```
services:

  db:

    image: postgres:15

    container_name: stock-db

    environment:

      - POSTGRES_USER=stock

      - POSTGRES_PASSWORD=stock

      - POSTGRES_DB=stock

    ports:

      - "5434:5432" #여기부분은 겹치지 않게 바꿔줘야함.

    volumes:

      - stock_data:/var/lib/postgresql/data

    restart: always

    networks:

      - stock-network

  

  stock-api:

    build: .

    container_name: stock-service

    volumes:

      - .:/app

    ports:

      - "9000:9000"

    environment:

      - DOMAIN=www.kimdonghee.com

      - DB_HOST=db

      - DB_PORT=5432

      - DB_NAME=stock

      - DB_USER=stock

      - DB_PASS=stock

    env_file:

      - .env

    depends_on:

      - db

    restart: always

    networks:

      - stock-network

  

volumes:

  stock_data:
  
networks:

  stock-network:

    driver: bridge
```


**그리고, .env 파일에 아래 코드를 추가해줘야 함.**

```
# 서버 설정

APP_HOST=0.0.0.0

APP_PORT=9000

  

# 도메인 설정

DOMAIN=www.kimdonghee.com

  

# 데이터베이스 설정

DB_HOST=localhost

DB_PORT=5434

DB_NAME=stock

DB_USER=stock

DB_PASSWORD=stock

  

# SQLAlchemy용 DATABASE_URL

DATABASE_URL=postgresql://stock:stock@localhost:5434/stock # 여기부분 추가해주기
```

**그리고 main.py에 추가해줘야 함.**

```
DATABASE_URL = os.getenv("DATABASE_URL") #여기 부분 추가해주기


if not DATABASE_URL: #여기 부분 추가해주기

    raise ValueError("DATABASE_URL environment variable is not set") #여기 부분 추가해주기
```

**그리고 stocks 테이블을 생성해야 한다. 그래야 DB와 연결되어 있는지를 알 수 있다.**이 테이블에 내가 원하는 기업들의 주가 정보를 크롤링한 값들이 들어간다.

1. 먼저, ddl을 해준다. (아래 코드는 model.py가 뭐냐에 따라 다를 수 있고, 커서한테 stock 테이블 생성해 달라고 해서 콘솔창에 짚어 넣으면 된다.)
```
CREATE TABLE stocks (

    id SERIAL PRIMARY KEY,

    company VARCHAR(50) NOT NULL,

    current_price VARCHAR(20) NOT NULL,

    change VARCHAR(20) NOT NULL,

    change_rate VARCHAR(20) NOT NULL,

    ask_price VARCHAR(20) NOT NULL,

    bid_price VARCHAR(20) NOT NULL,

    volume VARCHAR(20) NOT NULL,

    trading_value_mil VARCHAR(20) NOT NULL,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP

);
```

2. stocks 테이블에 데이터 더미 데이터 삽입하기 --dml

```
-- 삼성전자 주식 데이터 삽입

INSERT INTO stocks (

    company,

    current_price,

    change,

    change_rate,

    ask_price,

    bid_price,

    volume,

    trading_value_mil

) VALUES (

    '삼성전자',

    '55,200',

    '▲ 100',

    '+0.18%',

    '55,300',

    '55,200',

    '2,309,195',

    '127,391'

);


```

3. stock s테이블 데이터 조회하기 -- dql
```
SELECT

    company AS "종목명",

    current_price AS "현재가",

    change AS "전일대비",

    change_rate AS "등락률",

    ask_price AS "매도호가",

    bid_price AS "매수호가",

    volume AS "거래량",

    trading_value_mil AS "거래대금(백만)"

FROM stocks; #데이터 전체 확인하는 코드
```

**이렇게 더미 데이터를 삽입하였다. 그러면 테이블에 더미데이터가 들어간 것을 알 수 있다. 
###### 개념 정리: 데이터가 DB로 들어가는 것 insert라고 하고, DB에 있는 데이터를 sweagger에서 꺼내는 것을 fetch라고 한다. 

2. app 폴더 아래에 api 폴더 및 domain 폴더, foundation 및 platform 폴더 생성

3. api 폴더 및 domain 폴더 아래에 crawler 폴더 생성

4. api 폴더 밑에 서브라우터 생성(아래 참고하시오.)
	1. samsung_router.py
	2. hyundai_router.py
	3. jahwa_router.py

5.  메인 라우터 생성 및 등록

```main.py
# 라우터 생성

stock_router = APIRouter(prefix="/stock")

# 라우터 등록

app.include_router(stock_router)
```

6. 메인 라우터에 서브라우터 등록(임포트 주의)

```main.py
from app.api.crawler.samsung_router import router as samsung_router

from app.api.crawler.hyundai_router import router as hyundai_router

from app.api.crawler.jahwa_router import router as jahwa_router


# 서브 라우터 등록

app.include_router(samsung_router)

app.include_router(hyundai_router)

app.include_router(jahwa_router)
```

6.  domian 폴더 아래에 controllers, models, repositories, schemas, services 폴더 생성 및 각각의 파일 생성

7.  종목별로 서브라우터 생성하기로 했음.

![[Pasted image 20250418105319.png]]

8. 크롤링 할 때, 현재가, 전일대비, 등락률, 매도호가, 매수호가, 거래량, 거래대금(백만) 지표를 가지고 오고 싶어. **(내 폴더 구조를 보여주면서...)** 그리고 내 폴더 구조는 이미지와 같아. 다시 참고해서 코드 작성해줘. 코드 구현은 아래와 같아? 1. stock_scema.py 코드 구현 2. stock_model.py 코드 구현 3. stock_repository.py 코드 구현 4. stock_service.py 코드 구현 5. stock_controller.py 코드 구현 6. stock_router.py에 연결 시키기 메인 라우터까지 다시 코드 구현해서 보여줘

9. 난 데이터 베이스 postgresql을 사용할거야. 이 프로젝트에서는 **PostgreSQL**을 사용할 예정이니, 그에 맞게 **SQLAlchemy 기반 ORM 모델과 데이터베이스 설정**을 구성해줘.


#### 코드 구현
1. `app/domain/crawler/schemas/stock_schema.py`

```
from pydantic import BaseModel

class StockResponse(BaseModel):
    company: str
    current_price: str
    change: str
    change_rate: str
    ask_price: str
    bid_price: str
    volume: str
    trading_value_mil: str

    model_config = {
        "from_attributes": True  # <-- ✅ DB 모델을 스키마로 변환할 수 있게 해줌

```

###### 왜 `orm_mode = True`를 넣는가?
SQLAlchemy 모델 → Pydantic 스키마로 자동 변환하려면 반드시 필요합니다.

###### swagger에 찍히려면 아래 3가지 조건을 만족해야함

1. 서브 라우터가 `app.include_router()`로 등록되어 있어야 함. 
```
from app.api.crawler.samsung_router import router as samsung_router

app.include_router(samsung_router)  # ✅ 이 코드가 있어야 Swagger에 뜹니다.

```

2. 서브 라우터에 `prefix`, `tags`가 설정되어 있어야 함.
```
router = APIRouter(
    prefix="/stock/samsung",
    tags=["Samsung"]  # ← 여기에 명확한 그룹 이름을 주면 Swagger에 따로 뜹니다
)

```

3. 서브라우터에 데코레이터와 엔드포인트 함수 정의는 반드시 있어야 함.
	1. 엔드포인트에 명시적인 `@router.get/post(..., response_model=...)`이 있어야 함
```samsung_router.py
@router.post("", response_model=StockResponse)
async def crawl_and_save_samsung(db: Session = Depends(get_db)):
    ...
```


#### 코딩 기본 구조(해야할 일)
1. 폴더 구조 만들기
2. 헬로우 월드 띄우기
3. 데이터베이스에서 더미 데이터 집어 넣기
4. 스웨거 연결되었는지 확인하기



##### 코드 분석
```samsung_router.py
from fastapi import APIRouter, Depends, HTTPException

from sqlalchemy.ext.asyncio import AsyncSession

from app.platform.async_db import get_async_session

from app.domain.crawler.schemas.samsung_schema import SamsungStockResponse

from app.domain.crawler.services.samsung_service import SamsungStockService

  

router = APIRouter(prefix="/stock/samsung", tags=["Samsung"])

service = SamsungStockService()

  

#

@router.post("", response_model=SamsungStockResponse)

async def crawl_and_save_stock(db: AsyncSession = Depends(get_async_session)):

    try:

        stock = await service.crawl_and_save(db)

        return SamsungStockResponse.model_validate(stock)

    except Exception as e:

        raise HTTPException(status_code=500, detail=str(e))
```

라우터에서 리퀘스트랑 리스폰스랑 assyncsession이 만나요. 
리퀘스트와 리스폰스


프론트에서 엑시오스를 통해 리퀘스트가 왓는데, 왜 여기에 asyncsession이 있을까?.....


##### 새로 알게된 개념
1. 엔드 포인트 --> 서브 라우터에 있다. 
	1. 엔드포인트는 항상 `서브 router`에서 정의하고,  `controller`, `service`, `repository`는 그 뒤에서 로직과 DB를 처리합니다.
```
router = APIRouter(prefix="/stock/samsung", tags=["Samsung"]) --> 우리는 이 부분을 통해 딱 여기까지 공통 경로를 지정해 준것을 알 수 있다.이 부분은 엔드 포인트 전의 공통 경로를 말해줌. 

엔드 포인트 예시)
@router.post("", response_model=SamsungStockResponse) --> 이 부분이 엔드포인트이다. 
```



#### samsung_router.py에서 알게된 개념
```
@router.post("", response_model=SamsungStockResponse)
```

- @router.post("", --> 이 부분은 프론트에서 post 메소드 방식으로 http 요청을 보내는 부분을 의미함.

- response_model=SamsungStockResponse) --> 이 부분은 백엔드가 http응답을 프론트한테 할때 SamsungStockResponse 형식으로 보내준다는 것을 의미함.

```
async def crawl_and_save_stock(db: AsyncSession = Depends(get_async_session)):
```
- @router.post("", response_model=SamsungStockResponse)이 코드를 보면 프론트가 HTTP 요청을 통해 post 메서드로 크롤링 및 데이터 저장을 요청을 했고, 백엔드는 samsungstockresponse 형식으로 응답을 하겠다는 것은 이해가 됨. 
- crawl_and_save_stock이 함수를 실행할 때, get_async_session 이 함수를 무조건 실행하기로 했다. get_async_session은 fastAPI에 프론트 요청이 들어올 때마다 DB세션(연결선)을 자동으로 만들어주는 의존성 주입 함수이다. **db** 여기에서 이 부분은 진짜 DB가 아니라 DB 세션(DB와 연결되어 있는 통로)이다.
- db 세션을 만들어주는 공간은 asyncsessionlocal( )이고, sqlalchemy 내부에서 asyncsession 객체를 생성한다. 즉 asyncsessionlocal( )여기는 db세션을 만드는 공장이고, sqlalchemy 내부는 db 세션 객체를 생성하는 곳이다.
- 즉, DB세션은 요청마다 잠깐 연결되는 임시 통신선이고, fastAPI +sqlalchemy가 자동으로 만들고,자동으로 닫아준다.

- ###### 이해를 돕는 비유
	- async_sessionmaker: 세션을 만드는 **설계도 
	- AsyncSessionLocal(): 세션을 만드는 **공장**
	- SQLAlchemy : 내부|진짜로 세션을 만들어주는 **공장 기계 내부**
	- db: AsyncSession: |우리가 받은 세션 = **완성품**

#### DB 연결을 비동기 방식으로 하고 싶을때 프롬프팅하는 방법
- “FastAPI 기반의 비동기 웹 API를 구축하고 싶습니다.  데이터베이스 연결은 SQLAlchemy + asyncpg를 사용한 비동기 ORM 방식으로 처리해주세요.”
- 또는, “이 프로젝트는 FastAPI 기반 비동기 API 서비스이며,  SQLAlchemy + asyncpg 조합으로 PostgreSQL과 비동기적으로 연결됩니다.”

##### 세션 공장 만드는 곳
```async_db.py
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker

engine = create_async_engine(DATABASE_URL)

AsyncSessionLocal = async_sessionmaker(
    bind=engine,
    class_=AsyncSession,
    expire_on_commit=False
)

```
###### 위의 코드의 흐름 쉽게 정리하면!!!

1. FastAPI에 요청이 들어옴
    
2. `Depends(get_async_session)` 확인
    
3. FastAPI가 `get_async_session()` 실행
    
4. `AsyncSessionLocal()`을 통해 DB 세션 하나 생성
    
5. 그걸 `db` 변수에 넣어줌
    
6. 요청 처리 끝나면 세션 자동으로 close
###### 추가로 다운로드 해야할 라이브러리: asyncpg,aiohttp
- `asyncpg`는 PostgreSQL과 비동기 방식으로 통신할 수 있도록 도와주는 초고속 드라이버이다.(`psycopg2` → PostgreSQL 전통적인 "동기" 드라이버이니까, psycopg2는 다운로드해서는 안된다.,)
- 동기는 은행 창구 1개이고, 비동기는 번호표 뽑고 기다리면 직원이 자동으로 호출해서 병렬로 처리하는 것이다.



##### 선생님이 추가해주신 설명

- response(백엔드에서 응답되어질 객체) = request(프론트에서 요청한 객체) + 가공(컨트롤러, 서비스, 레파지토리까지 거친 과정 속에 생긴 것들) + DB
- DB는 움직이지 않고, 움직이는 것은 DB 세션이다.

#### 주의 사항
- 지금은 프론트에서 넘어오는 것들이 크롤링 요청 밖에 없으니까, request_body가 프론트에서 payload를 통해 넘어오지 않았다. payload에 담길 것들은 토큰, 검색어 등이 있을 수 있다.  근데 만약 프론트에서 request_body가 넘어오면 payload에 토큰, 검색어 등이 담겨져 올 것이다. 그래서 선생님은 request_body부분을 samsung_router.py에 만들라고 하셨다.......
#### 다음주에 확인해봐야할 것
1. DB 세션이 서비스, 컨트롤러, 레파지토리에 모두 다 연결되어 있다. 왜? 다 연결되어 있어? --> 찾아보기
2. 선생님은 request_body부분을 samsung_router.py에 만들라고 하셨다.......이게 search 기능인 것일까?
3. 내 로직을 확인해본 결과.....윗 줄의 삼성전자는 더미 데이터를  DB에 직접 주입을 한 결과이고, 두번째 줄 삼성전자는 매도호가, 매수호가, 거래량, 거래대금(백만)이 크롤링이 되지 않았다. 그 이유는 네이버 금융 사이트에 들어가서 확인해본 결과, F12를 눌렀을 때, 클래스 이름이 없었다. 이건 내 잘못이 아니라, 네이버가 이상하게 한 것이다. 아래 사진을 참조하면 알 수 있다. 오류가 없는 것을 알 수 있다. 근데 네이버 금융 사이트에 매도호가는 클래스가 없기 때문에 나는 크롤링을 할 수 없었다. 그렇다면 내가 해야할 일은 클래스 이름이 없는 것을 크롤링 하는 방법을 알아야 한다. 
![[Pasted image 20250418191323.png]]
4. ![[Pasted image 20250418191109.png]]


##### 새로 알게된 주의 사항

- .env 파일에서  `DATABASE_URL=postgresql+asyncpg://stock:stock@db:5432/stock` 이 부분에서 **`@db`로 설정한 이유는 바로 도커 컨테이너 간 통신 때문**이에요. DATABASE_URL은 도커 컨테이너 주소이다. 근데 내 docker-compose.yml을 보면 아래 사진처럼 **db**로 되어있다.
- ![[Pasted image 20250418191755.png]]
