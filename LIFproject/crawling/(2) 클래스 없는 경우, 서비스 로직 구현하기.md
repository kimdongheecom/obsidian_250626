

클래스 이름이 없는 경우 --> HTML 요소에 `class`나 `id` 속성이 없으면, `select_one()`이나 `select()`에서 **CSS 선택자**만으로 정확히 원하는 요소를 찾기 어렵습니다.  이럴 때는 **정확한 위치 기반 선택자**를 사용해야 해요.

해결방법
1. 맨 처음, td기반으로  index로 접근하였음. 하지만, .select()위치와 td 순서가 틀렸을 가능성이 높음.
--> `ask_price`, `bid_price`, `volume`, `trading_value_mil`가 `"-"`로 출력되는 이유는 HTML 구조와 선택자가 실제와 살짝 안 맞기 때문이에요.

2.  1번 방법이 해결되지 않았음. `td` 인덱스 접근보다 `tr:nth-of-type(x) td:nth-of-type(y)` 방식 활용 --> 정확하고 신뢰도가 높음


- 클래스 이름 없는 경우일 때, 크롤링 그냥 했음
```samsung_service.py(클래스 이름이 없는 경우 - 일부분에 대한 크롤링을 계속 해오지 못함 )
from sqlalchemy.ext.asyncio import AsyncSession

from app.domain.crawler.repositories.samsung_repository import SamsungStockRepository

import aiohttp

from bs4 import BeautifulSoup

  

class SamsungStockService:

    def __init__(self):

        self.repository = SamsungStockRepository()

  

    async def fetch_from_naver(self) -> dict:

        url = "https://finance.naver.com/item/main.naver?code=005930"

        headers = {"User-Agent": "Mozilla/5.0"}

  

        async with aiohttp.ClientSession() as session:

            async with session.get(url, headers=headers) as res:

                text = await res.text()

                soup = BeautifulSoup(text, "html.parser")

  

                # ✅ 현재가

                current_price = soup.select_one("p.no_today span.blind")

                if not current_price:

                    raise ValueError("❌ 현재가 정보를 찾을 수 없습니다.")

  

                # ✅ 전일대비

                change = soup.select_one("p.no_exday em span.blind")

                if not change:

                    raise ValueError("❌ 전일대비 정보를 찾을 수 없습니다.")

  

                # ✅ 등락률

                change_rate_list = soup.select("p.no_exday em span.blind")

                if len(change_rate_list) < 2:

                    raise ValueError("❌ 등락률 정보를 찾을 수 없습니다.")

                change_rate = change_rate_list[1]

  

                # ✅ 기타 정보는 위치 기반으로 추출 (HTML 구조 기준)

                td_list = soup.select("table.no_info tbody tr:nth-of-type(1) td")

  

                ask_price = td_list[0].select_one("span.blind") if len(td_list) > 0 else None

                bid_price = td_list[1].select_one("span.blind") if len(td_list) > 1 else None

  

                td_list_2 = soup.select("table.no_info tbody tr:nth-of-type(2) td")

                volume = td_list_2[0].select_one("span.blind") if len(td_list_2) > 0 else None

                trading_value_mil = td_list_2[1].select_one("span.blind") if len(td_list_2) > 1 else None

  

                # 결과 생성

                result = {

                    "company": "삼성전자",

                    "current_price": current_price.text.strip(),

                    "change": change.text.strip(),

                    "change_rate": change_rate.text.strip(),

                    "ask_price": ask_price.text.strip() if ask_price else "-",

                    "bid_price": bid_price.text.strip() if bid_price else "-",

                    "volume": volume.text.strip() if volume else "-",

                    "trading_value_mil": trading_value_mil.text.strip() if trading_value_mil else "-",

                }

  

                print("✅ 크롤링 결과:", result)

                return result

  

    async def crawl_and_save(self, db: AsyncSession):

        data = await self.fetch_from_naver()

        return await self.repository.save(db, data)
```

```samsung_service.py(td기반으로 index 접근한 방법)
from sqlalchemy.ext.asyncio import AsyncSession

from app.domain.crawler.repositories.samsung_repository import SamsungStockRepository

import aiohttp

from bs4 import BeautifulSoup

  

class SamsungStockService:

    def __init__(self):

        self.repository = SamsungStockRepository()

  

    async def fetch_from_naver(self) -> dict:

        url = "https://finance.naver.com/item/main.naver?code=005930"

        headers = {"User-Agent": "Mozilla/5.0"}

  

        async with aiohttp.ClientSession() as session:

            async with session.get(url, headers=headers) as res:

                text = await res.text()

                soup = BeautifulSoup(text, "html.parser")

  

                # ✅ 현재가

                current_price = soup.select_one("p.no_today span.blind")

                if not current_price:

                    raise ValueError("❌ 현재가 정보를 찾을 수 없습니다.")

  

                # ✅ 전일대비

                change = soup.select_one("p.no_exday em span.blind")

                if not change:

                    raise ValueError("❌ 전일대비 정보를 찾을 수 없습니다.")

  

                # ✅ 등락률

                change_rate_list = soup.select("p.no_exday em span.blind")

                if len(change_rate_list) < 2:

                    raise ValueError("❌ 등락률 정보를 찾을 수 없습니다.")

                change_rate = change_rate_list[1]

  

                # ✅ 매도호가, 매수호가 (위치 기반)

                td_list = soup.select("table.no_info tbody tr:nth-of-type(1) td")

                ask_price = td_list[0].select_one("span.blind") if len(td_list) > 0 else None

                bid_price = td_list[1].select_one("span.blind") if len(td_list) > 1 else None

  

                # ✅ 거래량, 거래대금 (위치 기반)

                td_list_2 = soup.select("table.no_info tbody tr:nth-of-type(2) td")

                volume = td_list_2[0].select_one("span.blind") if len(td_list_2) > 0 else None

                trading_value_mil = td_list_2[1].select_one("span.blind") if len(td_list_2) > 1 else None

  

                # ✅ 결과 딕셔너리

                result = {

                    "company": "삼성전자",

                    "current_price": current_price.text.strip(),

                    "change": change.text.strip(),

                    "change_rate": change_rate.text.strip(),

                    "ask_price": ask_price.text.strip() if ask_price else "-",

                    "bid_price": bid_price.text.strip() if bid_price else "-",

                    "volume": volume.text.strip() if volume else "-",

                    "trading_value_mil": trading_value_mil.text.strip() if trading_value_mil else "-",

                }

  

                print("✅ 크롤링 결과:", result)

                return result

  

    async def crawl_and_save(self, db: AsyncSession):

        data = await self.fetch_from_naver()

        return await self.repository.save(db, data)
```


** 아래 파일은 클래스 이름 없이 크롤링하기 위해, 위치 기반 CSS 선택자**를 활용해 `ask_price`, `bid_price`, `volume`, `trading_value_mil`까지 정확히 파싱되도록 수정한 **최종 `service.py` 전체 코드**
```samsung_service.py
from sqlalchemy.ext.asyncio import AsyncSession

from app.domain.crawler.repositories.samsung_repository import SamsungStockRepository

import aiohttp

from bs4 import BeautifulSoup

  

class SamsungStockService:

    def __init__(self):

        self.repository = SamsungStockRepository()

  

    async def fetch_from_naver(self) -> dict:

        url = "https://finance.naver.com/item/main.naver?code=005930"

        headers = {"User-Agent": "Mozilla/5.0"}

  

        async with aiohttp.ClientSession() as session:

            async with session.get(url, headers=headers) as res:

                text = await res.text()

                soup = BeautifulSoup(text, "html.parser")

  

                # ✅ 현재가

                current_price_tag = soup.select_one("p.no_today span.blind")

                if not current_price_tag:

                    raise ValueError("❌ 현재가 정보를 찾을 수 없습니다.")

  

                # ✅ 전일대비

                change_tag = soup.select_one("p.no_exday em span.blind")

                if not change_tag:

                    raise ValueError("❌ 전일대비 정보를 찾을 수 없습니다.")

  

                # ✅ 등락률

                change_rate_list = soup.select("p.no_exday em span.blind")

                if len(change_rate_list) < 2:

                    raise ValueError("❌ 등락률 정보를 찾을 수 없습니다.")

                change_rate_tag = change_rate_list[1]

  

                # ✅ 매도호가 (1행 1열)

                ask_price_tag = soup.select_one("table.no_info tbody tr:nth-of-type(1) td:nth-of-type(1) span.blind")

  

                # ✅ 매수호가 (1행 2열)

                bid_price_tag = soup.select_one("table.no_info tbody tr:nth-of-type(1) td:nth-of-type(2) span.blind")

  

                # ✅ 거래량 (2행 1열)

                volume_tag = soup.select_one("table.no_info tbody tr:nth-of-type(2) td:nth-of-type(1) span.blind")

  

                # ✅ 거래대금 (2행 2열)

                trading_value_tag = soup.select_one("table.no_info tbody tr:nth-of-type(2) td:nth-of-type(2) span.blind")

  

                # ✅ 결과 딕셔너리 생성

                result = {

                    "company": "삼성전자",

                    "current_price": current_price_tag.text.strip(),

                    "change": change_tag.text.strip(),

                    "change_rate": change_rate_tag.text.strip(),

                    "ask_price": ask_price_tag.text.strip() if ask_price_tag else "-",

                    "bid_price": bid_price_tag.text.strip() if bid_price_tag else "-",

                    "volume": volume_tag.text.strip() if volume_tag else "-",

                    "trading_value_mil": trading_value_tag.text.strip() if trading_value_tag else "-",

                }

  

                print("✅ 크롤링 결과:", result)

                return result

  

    async def crawl_and_save(self, db: AsyncSession):

        data = await self.fetch_from_naver()

        return await self.repository.save(db, data)
```



나는 네이버 금융 사이트에서 https://finance.naver.com/ 종목명을 검색창에 입력했을 때, 종목명, 현재가, 전일대비, 등락률, 매도호가, 매수호가, 거래량, 거래대금(백만) 컬럼을 크롤링하고 싶어. 

현재, 클래스가 없는 경우 --> 현재가, 매도호가, 매수호가, 거래량, 거래대금(백만) 현재가의 정확한 위치를 말해줄게. 
(정확한 위치를 말할 때, 뭐와 뭐 사이에 있다는 것과 시블링 관계에 뭐가 있는지, 그리고 부모 클래스가 뭔지.. 근데 부모 클래스를 보면 특정지을 수 없기 때문에, 여기서 프롬프팅할 때 의미가 없다. )

예) 현재가는 class='tit' 와 class="num2 up" 사이에 있고, 매도호가, 매수호가, 거래량, 거래대금(백만)은 tr 태그 안에 위치해 있어.

클래스 이름이 없는 경우는 httml 기반에서 정확한 위치를 말해주면 된다. (아래 사진 참고하기)
(매도호가는 num2 up과 시블링 관계이고, 부모 클래스 태그는 tr이다. 근데 tr이 너무 많기 때문에 특정지을 수 없다. )

![[Pasted image 20250421141004.png]]


지금 보니까 네이버 금융 크롤링 하는 것은 정적 크롤링인줄 알았는데, 동적 크롤링으로 바꿔야 한다.

왜냐하면 https://finance.naver.com/search/search.naver?query=%BB%EF%BC%BA%C0%FC%C0%DA&endUrl= 여기에 들어갔을 때, 바로 삼성전자가 안나오고, 삼성전자를 누르고 들어가야 하기 때문이다.





종목번호로 들어가는 것 --> 정적 크롤링
https://finance.naver.com/item/main.naver?code=005930



![[Pasted image 20250421142346.png]]


#### 정적 크롤링 vs 동적 크롤링

###### 정적 크롤링: 
- 사용하는 라이브러리: 
	- requests 또는 aiohttp
	- beautifulsoup(HTML 파싱용)
-  HTML이 **서버에서 렌더링된 상태로 바로 응답되는 페이지**를 크롤링하는 방식이야.
- 즉, 요청(`requests`, `aiohttp`)을 보내면 **데이터가 HTML에 그대로 담겨** 있는 거야.

```
from sqlalchemy.ext.asyncio import AsyncSession

from app.domain.crawler.repositories.samsung_repository import SamsungStockRepository

import aiohttp

from bs4 import BeautifulSoup

  

class SamsungStockService:

    def __init__(self):

        self.repository = SamsungStockRepository()

  

    async def fetch_from_naver(self) -> dict:

        url = "https://finance.naver.com/item/main.naver?code=005930"

        headers = {"User-Agent": "Mozilla/5.0"}

  

        async with aiohttp.ClientSession() as session:

            async with session.get(url, headers=headers) as res:

                text = await res.text()

                soup = BeautifulSoup(text, "html.parser")

  

                # ✅ 현재가

                current_price_tag = soup.select_one("p.no_today span.blind")

                if not current_price_tag:

                    raise ValueError("❌ 현재가 정보를 찾을 수 없습니다.")

  

                # ✅ 전일대비

                change_tag = soup.select_one("p.no_exday em span.blind")

                if not change_tag:

                    raise ValueError("❌ 전일대비 정보를 찾을 수 없습니다.")

  

                # ✅ 등락률

                change_rate_list = soup.select("p.no_exday em span.blind")

                if len(change_rate_list) < 2:

                    raise ValueError("❌ 등락률 정보를 찾을 수 없습니다.")

                change_rate_tag = change_rate_list[1]

  

                # ✅ 매도호가 (1행 1열)

                ask_price_tag = soup.select_one("table.no_info tbody tr:nth-of-type(1) td:nth-of-type(1) span.blind")

  

                # ✅ 매수호가 (1행 2열)

                bid_price_tag = soup.select_one("table.no_info tbody tr:nth-of-type(1) td:nth-of-type(2) span.blind")

  

                # ✅ 거래량 (2행 1열)

                volume_tag = soup.select_one("table.no_info tbody tr:nth-of-type(2) td:nth-of-type(1) span.blind")

  

                # ✅ 거래대금 (2행 2열)

                trading_value_tag = soup.select_one("table.no_info tbody tr:nth-of-type(2) td:nth-of-type(2) span.blind")

  

                # ✅ 결과 딕셔너리 생성

                result = {

                    "company": "삼성전자",

                    "current_price": current_price_tag.text.strip(),

                    "change": change_tag.text.strip(),

                    "change_rate": change_rate_tag.text.strip(),

                    "ask_price": ask_price_tag.text.strip() if ask_price_tag else "-",

                    "bid_price": bid_price_tag.text.strip() if bid_price_tag else "-",

                    "volume": volume_tag.text.strip() if volume_tag else "-",

                    "trading_value_mil": trading_value_tag.text.strip() if trading_value_tag else "-",

                }

  

                print("✅ 크롤링 결과:", result)

                return result

  

    async def crawl_and_save(self, db: AsyncSession):

        data = await self.fetch_from_naver()

        return await self.repository.save(db, data)
```
###### 동적 크롤링:
- 사용하는 라이브러리:
	- seslenium(실제 브라우저처럼 렌더링)
	- Playwright(요즘 각광받는 대안)
- 웹 페이지가 **JavaScript를 통해 클라이언트에서 동적으로 HTML을 생성**하는 경우 사용해.

- 좀 더 쉽게 설명하자면, html 코드에 검색 결과가 처음부터 들어있지 않고, 브라우저가 페이지를 불러온 뒤 javascript가 실행되어 그 실행 결과로 테이블이 만들어진다는 것을 의미함.
    
- 예: 스크롤하면 계속 로딩되는 주식 페이지, 버튼 클릭 시 테이블 업데이트 등


###### 요약
![[Pasted image 20250421143350.png]]

![[Pasted image 20250421143621.png]]



###### 동적 크롤링할 때 필요한 프롬프팅
- Selenium + FastAPI + PostgreSQL

동적으로 크롤링 할 경우 -->  패키지 설치

```
pip install selenium webdriver-manager

```



