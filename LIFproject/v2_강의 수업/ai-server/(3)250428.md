

#### 나이브 베이즈 정리: 토너먼트라고 생각해라.

- 처음에는 어떤 믿음(선험적 확률)이 있었고, 이후 새로운 증거(데이터)가 주어졌고, 그걸 통해 최종 판단(사후 확률)을 갱신하는 것이다.
	- 선험적 확률: 50%(최초의 믿음) - 사전 확률
	- 새로운 증거: 예시) 무료, 광고
	- 최종 판단: -사후 확률
-  사후 확률이 계속 업데이트 된다. --> 정확도가 높아진다.
- 예시) 타이타닉 계산하는 것, feature의 개수에 따라 루프를 돌린다.
- 처음 믿음이 있고, 새로운 사실이 생겼을 때, 그걸 바탕으로 다시 생각해보는 과정이라고 볼 수 있다.


KNN 

###### 에포크
- 머신러닝에서 머신러닝 모델이 전체 데이터셋을 한 번 **다** 학습하는 과정"을 의미함.


#### 교차 검증 (cross-validation): 리그라고 생각해라.
- 리그 방식에 가깝다.
- 각각을 서로 다 붙여본다. 그러면 시간이 많이 걸린다. 하지만 정확도는 높다.
-  cross 는 리그 방식이라고 생각하면 된다. 함수 예시: croee_var_score



비즈니스를 한다면, 정확도와 속도 중에 가중치를 두는 것은 정확도에 가중치를 둔다. 

타이타닉에서 정확도만 따졌다. 



#### 결정트리 vs 랜덤 포레스트

- 결정트리: 결정된 나무가 있다. 선택지가 하나만 있는 것(엄청 빠르다)

	- 전문가 한명이 결정하는 것, 최종 결정자, 즉 알고리즘의 맨 마지막에 있어야 함. 결정 알고리즘이다.

- 랜덤 포레스트: 결정해야할 나무가 있다. 선택지가 여러 개가 있다.(엄청 느리다)

	- 전문가 여러명이 다수결로 결정하는 것을 의미함. 

**결정트리는 하나의 질문 경로로 예측하고, 랜덤포레스트는 여러 트리를 만들어 평균내거나 다수결로 더 똑똑하게 예측한다.**"
#### XGBoost vs LightGBM: 기울기의 차이를 의미함. 기울기는 오차를 의미한다. 
- 기울기는 **어디를, 얼마나 많이 고쳐야 하지?**"를 알려주는 게 바로 **기울기**

##### XGBoost란(기울기가 급하다): 속도보다는 정확도가 필요할 때 쓴다.
- **개념**:  
    → "Extreme Gradient Boosting"의 줄임말이야.  
    → **여러 개의 결정트리**를 **순차적으로** 학습시키면서,  
    → 앞 트리가 만든 에러(오차)를 다음 트리가 계속 **보완**하도록 학습하는 방식이야.

#### LightGBM이란(기울기가 완만하다): 속도가 필요할 때 쓴다.
-  **개념**:  
	→ "Light Gradient Boosting Machine"의 줄임말이야.  
	→ **XGBoost보다 더 빠르고 가벼운** 부스팅 기법이야.  
	→ 특히 **대용량 데이터**에서 **속도**와 **메모리 사용량**을 확 줄였어.


##### **쳇 지피티 질문. 머신러닝에서 클래스 이름 svm이 뭐고, 이것에 대응하는 알고리즘이 뭐야?**

머신 - 알고리즘이 여러개, 즉 패키지를 말한다.

#### SVM(서포트 벡터 머신): 

- support: 
	- 여러 데이터들이 있지만, 가장 직접적으로 영향을 주는 데이터를 support라고 한다.
	-  라벨과 상관관계가 높은 관계를 가진 데이터를 의미함.
	- 
-  support vector:
	- support가 여러개를 의미한다. support vector 평면 사이로 구분이 되어진다. 
	- vector: 방향을 가지고 있다. 
- svm: 선택된 자들만 갖다 놓음 ex) 

#### 사이킷런에서 svm의 클래스 이름이 왜 svc( )인 거야?

|클래스 이름|역할|
|---|---|
|`SVC()`|**분류(Classification)** 를 위한 SVM|
|`SVR()`|**회귀(Regression)** 를 위한 SVM|
|`LinearSVC()`|**선형 분류**만 하는 빠른 버전|
|`LinearSVR()`|**선형 회귀**만 하는 빠른 버전|
